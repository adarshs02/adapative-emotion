#!/usr/bin/env python3
"""
Baseline runner: EmoPatientMulti × Base Model via OpenRouter (no EmoBIRD pipeline)

- Reads multi-turn scenarios from datasets/EmoPatientMulti/scenarios.json
- For each dialogue, processes turns sequentially with conversation history
- Each assistant turn is generated by direct API call to the base model
- No EmoBIRD pipeline - just context + conversation history → model response
- Saves per-dialogue JSON results under EmoBIRDv2/eval_results/emopatient_multiturn_basemodel

Usage examples:
  export OPENROUTER_API_KEY=...  # required
  python EmoBIRDv2/eval_scripts/run_emopatient_multiturn_basemodel.py --limit 2
  python EmoBIRDv2/eval_scripts/run_emopatient_multiturn_basemodel.py --limit 10 --workers 5
  python EmoBIRDv2/eval_scripts/run_emopatient_multiturn_basemodel.py --model openai/gpt-4o-mini --limit 3
"""

from __future__ import annotations

import argparse
import json
import os
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import requests

# Repo paths
SCRIPT_DIR = Path(__file__).resolve().parent
REPO_ROOT = SCRIPT_DIR.parent.parent
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

# EmoBIRDv2 constants (for defaults and timeouts)
from EmoBIRDv2.utils.constants import (
    OPENROUTER_API_KEY,
    MODEL_NAME as DEFAULT_MODEL,
    MODEL_TEMPERATURE as DEFAULT_TEMPERATURE,
    OPENROUTER_CONNECT_TIMEOUT,
    OPENROUTER_READ_TIMEOUT,
)

OPENROUTER_URL = "https://openrouter.ai/api/v1/chat/completions"


def ensure_api_key() -> None:
    if not OPENROUTER_API_KEY:
        raise RuntimeError("OPENROUTER_API_KEY environment variable is not set")


def load_multiturn_scenarios(path: Path) -> List[Dict[str, Any]]:
    """Load multi-turn scenarios from JSON file."""
    with path.open("r", encoding="utf-8") as f:
        data = json.load(f)
    if not isinstance(data, list):
        raise ValueError(f"Expected list of scenarios in {path}, got {type(data)}")
    return data


def build_system_message(dialogue: Dict[str, Any]) -> str:
    """Build system message with clinical context."""
    parts = []
    
    if dialogue.get("diagnosis"):
        parts.append(f"Diagnosis: {dialogue['diagnosis']}")
    if dialogue.get("treatment_plan"):
        parts.append(f"Treatment Plan: {dialogue['treatment_plan']}")
    if dialogue.get("narrative"):
        parts.append(f"Patient Background: {dialogue['narrative']}")
    
    parts.append("\nYou are an empathetic, wise oncology assistant. Provide concise, practical, medically-grounded responses.")
    
    return "\n".join(parts)


def call_openrouter_chat(
    *,
    messages: List[Dict[str, str]],
    api_key: str,
    model: str,
    temperature: float,
    max_tokens: int,
    connect_timeout: int,
    read_timeout: int,
) -> str:
    """Call OpenRouter chat completion API with message history."""
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }

    payload = {
        "model": model,
        "messages": messages,
        "temperature": float(temperature),
        "max_tokens": int(max_tokens),
    }
    
    resp = requests.post(
        OPENROUTER_URL,
        headers=headers,
        data=json.dumps(payload),
        timeout=(connect_timeout, read_timeout),
    )
    resp.raise_for_status()
    data = resp.json()
    
    if isinstance(data, dict) and data.get("error"):
        raise RuntimeError(f"OpenRouter error: {data.get('error')}")
    
    try:
        return (data["choices"][0]["message"]["content"] or "").strip()
    except Exception:
        return ""


def process_dialogue(
    dialogue: Dict[str, Any],
    args: argparse.Namespace,
    dialogue_idx: int,
) -> Dict[str, Any]:
    """
    Process a single multi-turn dialogue with the base model.
    Each assistant turn is generated by direct API call with conversation history.
    """
    dialogue_id = dialogue.get("dialogue_id", f"D{dialogue_idx+1}")
    turns = dialogue.get("turns", [])
    
    print(f"\n[{dialogue_id}] Processing {len(turns)} turns...", file=sys.stderr)
    
    # Build system message with clinical context (used for all turns)
    system_context = build_system_message(dialogue)
    
    # Build message array with system context
    messages: List[Dict[str, str]] = [{"role": "system", "content": system_context}]
    
    # Results for this dialogue
    completed_turns = []
    turn_responses = []
    
    for turn_idx, turn in enumerate(turns):
        role = turn.get("role")
        text = turn.get("text", "").strip()
        
        if role == "patient":
            # Patient turn - add to message history
            messages.append({"role": "user", "content": text})
            completed_turns.append({"role": "patient", "text": text})
            print(f"  Turn {turn_idx+1}: Patient message added to context", file=sys.stderr)
            
        elif role == "assistant":
            # Assistant turn - call model with conversation history
            print(f"  Turn {turn_idx+1}: Generating assistant response...", file=sys.stderr)
            
            # Call model with retries
            max_retries = max(0, int(getattr(args, "turn_retries", 1)))
            response_text = ""
            
            for retry in range(max_retries + 1):
                if retry > 0:
                    print(f"    Retry {retry}/{max_retries} (empty output)...", file=sys.stderr)
                
                try:
                    response_text = call_openrouter_chat(
                        messages=messages,
                        api_key=OPENROUTER_API_KEY or "",
                        model=args.model,
                        temperature=args.temperature,
                        max_tokens=args.max_tokens,
                        connect_timeout=int(args.openrouter_connect_timeout),
                        read_timeout=int(args.openrouter_read_timeout),
                    )
                    
                    if args.log_raw and response_text:
                        trunc = (response_text[:500] + "...[truncated]") if len(response_text) > 500 else response_text
                        print(f"    Raw: {trunc}", file=sys.stderr)
                
                except Exception as e:
                    print(f"    Error: {e}", file=sys.stderr)
                    response_text = ""
                
                if response_text.strip():
                    break
                time.sleep(0.3)
            
            # Add assistant response to message history
            messages.append({"role": "assistant", "content": response_text})
            completed_turns.append({"role": "assistant", "text": response_text})
            
            # Record response metadata
            turn_responses.append({
                "turn_index": turn_idx + 1,
                "role": "assistant",
                "response": response_text,
                "retries_used": retry,
            })
            
            print(f"    Generated: {response_text[:100]}{'...' if len(response_text) > 100 else ''}", file=sys.stderr)
    
    return {
        "dialogue_id": dialogue_id,
        "diagnosis": dialogue.get("diagnosis"),
        "treatment_plan": dialogue.get("treatment_plan"),
        "narrative": dialogue.get("narrative"),
        "turns": completed_turns,
        "turn_responses": turn_responses,
    }


def process_dialogue_task(task_info: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    Wrapper for processing a single dialogue with file I/O.
    Used for parallel processing.
    """
    dialogue = task_info["dialogue"]
    args = task_info["args"]
    idx = task_info["idx"]
    out_dir = task_info["out_dir"]
    run_id = task_info["run_id"]
    
    dialogue_id = dialogue.get("dialogue_id", f"D{idx+1}")
    
    # Check for existing results if resume is enabled
    if args.resume:
        existing = list(out_dir.glob(f"{dialogue_id}_basemodel_*.json"))
        if existing:
            print(f"[skip] {dialogue_id} already has results (resume)", file=sys.stderr)
            return None
    
    # Process the dialogue
    result = process_dialogue(dialogue, args, idx)
    
    # Write individual dialogue result
    out_path = out_dir / f"{dialogue_id}_basemodel_{run_id}.json"
    with out_path.open("w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"[done] {dialogue_id} -> {out_path}", file=sys.stderr)
    
    return result


def main() -> None:
    parser = argparse.ArgumentParser(description="Run base model on multi-turn EmoPatient dialogues (OpenRouter)")
    parser.add_argument("--data", type=str, default=str(REPO_ROOT / "datasets" / "EmoPatientMulti" / "scenarios.json"), help="Path to multi-turn scenarios JSON")
    parser.add_argument("--start", type=int, default=0, help="Start index (0-based) for dialogues")
    parser.add_argument("--limit", type=int, default=None, help="Process only N dialogues")
    
    parser.add_argument("--model", type=str, default=DEFAULT_MODEL, help="OpenRouter model name (easily switch)")
    parser.add_argument("--temperature", type=float, default=DEFAULT_TEMPERATURE, help="Sampling temperature")
    parser.add_argument("--max-tokens", type=int, default=2048, help="Max new tokens for each turn")
    
    parser.add_argument("--turn-retries", "--qa-retries", type=int, default=1, dest="turn_retries", help="Re-run API call if output is empty")
    parser.add_argument("--openrouter-connect-timeout", type=int, default=OPENROUTER_CONNECT_TIMEOUT, help="Connect timeout (s)")
    parser.add_argument("--openrouter-read-timeout", type=int, default=OPENROUTER_READ_TIMEOUT, help="Read timeout (s)")
    
    parser.add_argument("--output-dir", type=str, default=str(REPO_ROOT / "EmoBIRDv2" / "eval_results" / "emopatient_multiturn_basemodel"), help="Output directory")
    parser.add_argument("--resume", action="store_true", help="Skip dialogues that already have output files")
    parser.add_argument("--log-raw", action="store_true", help="Print truncated raw model outputs to stderr")
    parser.add_argument("--workers", type=int, default=1, help="Number of parallel workers for processing dialogues. Default: 1 (sequential)")

    args = parser.parse_args()

    ensure_api_key()

    data_path = Path(args.data)
    out_dir = Path(args.output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    dialogues = load_multiturn_scenarios(data_path)
    start = max(0, int(args.start))
    end = len(dialogues) if args.limit is None else min(len(dialogues), start + int(args.limit))
    run_dialogues = list(enumerate(dialogues[start:end], start=start))

    run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    print(f"Running EmoPatientMulti × BaseModel: {len(run_dialogues)} dialogues | model={args.model} | run_id={run_id}", file=sys.stderr)

    all_results = []
    workers = max(1, int(args.workers))

    # Prepare tasks for all dialogues
    tasks = []
    for idx, dialogue in run_dialogues:
        tasks.append({
            "dialogue": dialogue,
            "args": args,
            "idx": idx,
            "out_dir": out_dir,
            "run_id": run_id,
        })

    if workers == 1:
        # Sequential processing
        print(f"Processing {len(tasks)} dialogues sequentially...", file=sys.stderr)
        for task in tasks:
            result = process_dialogue_task(task)
            if result is not None:
                all_results.append(result)
            time.sleep(0.5)  # Gentle pacing
    else:
        # Parallel processing with ThreadPoolExecutor
        print(f"Processing {len(tasks)} dialogues with {workers} workers...", file=sys.stderr)
        with ThreadPoolExecutor(max_workers=workers) as executor:
            # Submit all tasks
            future_to_task = {executor.submit(process_dialogue_task, task): i for i, task in enumerate(tasks)}
            
            # Collect results as they complete
            completed = 0
            for future in as_completed(future_to_task):
                task_idx = future_to_task[future]
                try:
                    result = future.result()
                    if result is not None:
                        all_results.append(result)
                except Exception as exc:
                    dialogue_id = tasks[task_idx]["dialogue"].get("dialogue_id", f"D{task_idx+1}")
                    print(f"[error] {dialogue_id} generated exception: {exc}", file=sys.stderr)
                finally:
                    completed += 1
                    print(f"Progress: {completed}/{len(tasks)} dialogues completed", file=sys.stderr)

    # Write consolidated results
    consolidated_path = out_dir / f"emopatient_multiturn_basemodel_{run_id}.json"
    consolidated = {
        "run_id": run_id,
        "model": args.model,
        "temperature": args.temperature,
        "dialogues": all_results,
    }
    with consolidated_path.open("w", encoding="utf-8") as f:
        json.dump(consolidated, f, ensure_ascii=False, indent=2)
    print(f"\n[done] Consolidated results -> {consolidated_path}", file=sys.stderr)


if __name__ == "__main__":
    main()
