#!/usr/bin/env python3
"""
Thin wrapper to run EQBench3 from this repository.

It imports `run_eq_bench3()` from `datasets/eqbench3/core/benchmark.py` and wires up
paths for leaderboard data and local result files relative to the project root.

Environment variables for API keys/URLs (used by `utils/api.APIClient` inside EQBench3):
- TEST_API_KEY / TEST_API_URL (preferred for test model)
- JUDGE_API_KEY / JUDGE_API_URL (preferred for judge model)
- OPENROUTER_API_KEY / OPENROUTER_BASE_URL (global fallbacks)

Example:
  python scripts/eqbench3_run.py \
      --model-name "meta-llama/Llama-3.1-8B-Instruct" \
      --api-model-id "gpt-4o-mini" \
      --judge-model "gpt-4o-mini" \
      --iterations 1 \
      --threads 4 \
      --local-dir results/eqbench3

Notes:
- When ELO or Rubric is enabled, `--judge-model` is required by EQBench3.
- Local files are written only to the "local" directory; leaderboard files are read-only.
"""
from __future__ import annotations

import argparse
import os
from pathlib import Path
import sys
import logging

"""Import wiring

We avoid importing via 'datasets.eqbench3' because many environments have a
PyPI package named 'datasets' installed, which masks our local folder. To
prevent this, we prepend the EQBench3 package root to sys.path and import the
modules directly as 'core' and 'utils'.
"""
# Ensure repository root and EQBench3 package root are on sys.path
THIS_FILE = Path(__file__).resolve()
REPO_ROOT = THIS_FILE.parent.parent
EQB3_ROOT = REPO_ROOT / "datasets" / "eqbench3"

for p in (str(EQB3_ROOT), str(REPO_ROOT)):
    if p not in sys.path:
        sys.path.insert(0, p)

# Import benchmark entrypoint and constants from EQBench3 package root
from core.benchmark import run_eq_bench3
import utils.constants as C


def _abs_path(p: Path | str) -> str:
    return str((REPO_ROOT / p).resolve()) if not str(p).startswith(os.sep) else str(Path(p).resolve())


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Run EQBench3 benchmark via repo wrapper.")

    # Model identifiers
    parser.add_argument("--model-name", required=True,
                        help="Logical display name for the model (appears in results and ELO table).")
    parser.add_argument("--api-model-id", required=True,
                        help="Model ID string passed to the API client (e.g., provider/model-id).")

    # Judge / feature flags
    parser.add_argument("--judge-model", default=None,
                        help="Judge model ID for ELO and rubric. Required unless both --no-elo and --no-rubric are set.")
    parser.add_argument("--no-elo", action="store_true", help="Disable ELO analysis.")
    parser.add_argument("--no-rubric", action="store_true", help="Disable rubric scoring.")
    parser.add_argument("--truncate-for-rubric", action="store_true",
                        help="Truncate transcript text before rubric to fit within judge limits.")

    # Run control
    parser.add_argument("--iterations", type=int, default=1, help="Number of benchmark iterations (default: 1).")
    parser.add_argument("--threads", type=int, default=4, help="Thread count for scenario execution (default: 4).")
    parser.add_argument("--run-id", default=None,
                        help="Optional run ID prefix (otherwise autogenerated). Reuse to resume.")

    # Paths
    parser.add_argument("--local-dir", default="results/eqbench3",
                        help="Directory to write local run and ELO files (default: results/eqbench3).")
    parser.add_argument("--leaderboard-data-dir", default="datasets/eqbench3/data",
                        help="Directory with canonical leaderboard .json.gz files (default: datasets/eqbench3/data).")

    return parser.parse_args()


def main() -> int:
    args = parse_args()

    # Resolve directories
    local_dir = REPO_ROOT / args.local_dir
    local_dir.mkdir(parents=True, exist_ok=True)
    leaderboard_dir = REPO_ROOT / args.leaderboard_data_dir

    # Build file paths
    local_runs_file = local_dir / C.DEFAULT_LOCAL_RUNS_FILE
    local_elo_file = local_dir / C.DEFAULT_LOCAL_ELO_FILE
    leaderboard_runs_file = leaderboard_dir / Path(C.CANONICAL_LEADERBOARD_RUNS_FILE).name
    leaderboard_elo_file = leaderboard_dir / Path(C.CANONICAL_LEADERBOARD_ELO_FILE).name

    # Basic validation / logging hints
    run_elo = not args.no_elo
    run_rubric = not args.no_rubric
    if (run_elo or run_rubric) and not args.judge_model:
        print("ERROR: --judge-model is required when ELO and/or rubric scoring is enabled.")
        return 2

    # Friendly env hints
    test_key = os.getenv("TEST_API_KEY") or os.getenv("OPENROUTER_API_KEY")
    judge_key = os.getenv("JUDGE_API_KEY") or os.getenv("OPENROUTER_API_KEY")
    if not test_key:
        logging.warning("TEST_API_KEY/OPENROUTER_API_KEY not set. API calls for test model may fail.")
    if (run_elo or run_rubric) and not judge_key:
        logging.warning("JUDGE_API_KEY/OPENROUTER_API_KEY not set. Judge calls will fail if enabled.")

    # Run benchmark
    run_key = run_eq_bench3(
        model_name=args.model_name,
        api_model_id=args.api_model_id,
        # Files
        local_runs_file=str(local_runs_file),
        local_elo_file=str(local_elo_file),
        leaderboard_runs_file=str(leaderboard_runs_file),
        leaderboard_elo_file=str(leaderboard_elo_file),
        # Controls
        num_threads=args.threads,
        run_id=args.run_id,
        iterations=args.iterations,
        save_interval=2,
        # Features
        run_elo=run_elo,
        run_rubric=run_rubric,
        judge_model=args.judge_model,
        redo_judging=False,
        truncate_for_rubric=args.truncate_for_rubric,
    )

    print(f"EQBench3 run started. Run key: {run_key}")
    print(f"Local runs file: {local_runs_file}")
    print(f"Local ELO file:  {local_elo_file}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
