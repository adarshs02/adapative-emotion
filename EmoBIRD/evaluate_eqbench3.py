#!/usr/bin/env python3
"""
Run EQBench3 evaluation from within the EmoBIRD repository.

This script wires up sys.path so that our local EQBench3 package (under
`datasets/eqbench3/`) is imported directly, avoiding clashes with any
installed `datasets` packages. It also provides a convenient flag to route
all "test" model calls through the local EmoBIRD vLLM wrapper rather than
remote HTTP APIs.

Examples:
  python EmoBIRD/evaluate_eqbench3.py \
      --model-name "EmoBIRD-Llama-8B" \
      --api-model-id "gpt-4o-mini" \
      --judge-model "gpt-4o-mini" \
      --iterations 1 \
      --threads 4 \
      --local-dir EmoBIRD/eval_results/eqbench3 \
      --use-emobird-vllm

Notes:
- If --use-emobird-vllm is ON (default), the 'test' API client inside
  EQBench3 will delegate generation to EmoBIRD.VLLMWrapper. Remote API keys
  are not required for the test model in that case.
- Judge model calls (ELO/rubric) continue to use HTTP. Set JUDGE_API_KEY
  and JUDGE_API_URL (or OPENAI_API_KEY/OPENAI_API_URL) accordingly when
  enabling ELO or rubric.
"""
from __future__ import annotations

import argparse
import os
import sys
import logging
from pathlib import Path

# ---------- Import wiring to prefer local EQBench3 over site packages ---------
THIS_FILE = Path(__file__).resolve()
REPO_ROOT = THIS_FILE.parent.parent            # repo root
EQB3_ROOT = REPO_ROOT / "datasets" / "eqbench3"  # eqbench3 package root

for p in (str(EQB3_ROOT), str(REPO_ROOT)):
    if p not in sys.path:
        sys.path.insert(0, p)

# After path fixups, import EQBench3 components
from core.benchmark import run_eq_bench3
import utils.constants as C


def _abs_path(p: Path | str) -> str:
    return str((REPO_ROOT / p).resolve()) if not str(p).startswith(os.sep) else str(Path(p).resolve())


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Run EQBench3 (from EmoBIRD).")

    # Model identifiers
    parser.add_argument("--model-name", required=True,
                        help="Logical display name for the model (appears in results and ELO table).")
    parser.add_argument("--api-model-id", required=True,
                        help="Model ID string passed to the API client (e.g., provider/model-id).")

    # Judge / feature flags
    parser.add_argument("--judge-model", default=None,
                        help="Judge model ID for ELO and rubric. Required unless both --no-elo and --no-rubric are set.")
    parser.add_argument("--no-elo", action="store_true", help="Disable ELO analysis.")
    parser.add_argument("--no-rubric", action="store_true", help="Disable rubric scoring.")
    parser.add_argument("--truncate-for-rubric", action="store_true",
                        help="Truncate transcript text before rubric to fit within judge limits.")

    # Run control
    parser.add_argument("--iterations", type=int, default=1, help="Number of benchmark iterations (default: 1).")
    parser.add_argument("--threads", type=int, default=4, help="Thread count for scenario execution (default: 4).")
    parser.add_argument("--run-id", default=None,
                        help="Optional run ID prefix (otherwise autogenerated). Reuse to resume.")

    # Paths
    parser.add_argument("--local-dir", default="EmoBIRD/eval_results/eqbench3",
                        help="Directory to write local run and ELO files (default: EmoBIRD/eval_results/eqbench3).")
    parser.add_argument("--leaderboard-data-dir", default="datasets/eqbench3/data",
                        help="Directory with canonical leaderboard .json.gz files (default: datasets/eqbench3/data).")

    # EmoBIRD integration toggle
    on = parser.add_mutually_exclusive_group()
    on.add_argument("--use-emobird-vllm", dest="use_emobird_vllm", action="store_true",
                    help="Route 'test' model through EmoBIRD VLLMWrapper (default).")
    on.add_argument("--no-use-emobird-vllm", dest="use_emobird_vllm", action="store_false",
                    help="Disable EmoBIRD VLLM routing; use HTTP API for 'test' model.")
    parser.set_defaults(use_emobird_vllm=True)

    return parser.parse_args()


def main() -> int:
    args = parse_args()

    # Toggle EmoBIRD delegation for 'test' API client in EQBench3
    os.environ["EQB3_USE_EMOBIRD_VLLM"] = "1" if args.use_emobird_vllm else "0"

    # Resolve directories relative to repo root
    local_dir = REPO_ROOT / args.local_dir
    local_dir.mkdir(parents=True, exist_ok=True)
    leaderboard_dir = REPO_ROOT / args.leaderboard_data_dir

    # Build file paths
    local_runs_file = local_dir / C.DEFAULT_LOCAL_RUNS_FILE
    local_elo_file = local_dir / C.DEFAULT_LOCAL_ELO_FILE
    leaderboard_runs_file = leaderboard_dir / Path(C.CANONICAL_LEADERBOARD_RUNS_FILE).name
    leaderboard_elo_file = leaderboard_dir / Path(C.CANONICAL_LEADERBOARD_ELO_FILE).name

    # Env hints
    if args.use_emobird_vllm:
        logging.info("Using EmoBIRD VLLMWrapper for 'test' model. TEST_API_KEY not required.")
    else:
        test_key = os.getenv("TEST_API_KEY") or os.getenv("OPENAI_API_KEY")
        if not test_key:
            logging.warning("TEST_API_KEY/OPENAI_API_KEY not set. API calls for test model will fail.")
    judge_key = os.getenv("JUDGE_API_KEY") or os.getenv("OPENAI_API_KEY")
    run_elo = not args.no_elo
    run_rubric = not args.no_rubric
    if (run_elo or run_rubric) and not judge_key:
        logging.warning("JUDGE_API_KEY/OPENAI_API_KEY not set. Judge calls will fail if enabled.")

    # Run benchmark
    run_key = run_eq_bench3(
        model_name=args.model_name,
        api_model_id=args.api_model_id,
        # Files
        local_runs_file=str(local_runs_file),
        local_elo_file=str(local_elo_file),
        leaderboard_runs_file=str(leaderboard_runs_file),
        leaderboard_elo_file=str(leaderboard_elo_file),
        # Controls
        num_threads=args.threads,
        run_id=args.run_id,
        iterations=args.iterations,
        save_interval=2,
        # Features
        run_elo=run_elo,
        run_rubric=run_rubric,
        judge_model=args.judge_model,
        redo_judging=False,
        truncate_for_rubric=args.truncate_for_rubric,
    )

    print(f"EQBench3 run started. Run key: {run_key}")
    print(f"Local runs file: {local_runs_file}")
    print(f"Local ELO file:  {local_elo_file}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
