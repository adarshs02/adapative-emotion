import sys
import re
import json
import datetime
from pathlib import Path
import torch
import pandas as pd
from tqdm import tqdm

# --- BirdModel Integration --- 
script_path = Path(__file__).resolve()
project_root = script_path.parent
sys.path.insert(0, str(project_root))
from bird_model import BirdModel

def normalize_text(text):
    if not isinstance(text, str):
        return ""
    return text.strip().lower()

# ---- Main ----
if __name__ == "__main__":
    # ---- Script Setup & Initializations ----
    bird_config = {
        "scenarios_file": "scenarios-emotionbench.json",
        "cpt_dir": "cpts-lora-emotionbench", # Use the CPTs generated by the LoRA model
    }
    print("Initializing BirdModel with LoRA CPTs...")
    model = BirdModel(bird_config)
    print("BirdModel initialized.")

    emobench_root = Path("/mnt/shared/adarsh/EmoBench")
    DATA_DIR = emobench_root / "data"
    
    # --- Logging Setup ---
    LOG_DIR = project_root / "logs"
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")

    task_accuracies = {}

    for task in ["EA", "EU"]:
        run_logs = []
        data_path = str(DATA_DIR / f"{task}.jsonl")
        if not Path(data_path).exists():
            print(f"Warning: Data file not found at {data_path}. Skipping.")
            continue

        df = pd.read_json(data_path, lines=True, encoding="utf-8")
        df = df[df["language"] == "en"]
        print(f"\nEvaluating {task}-en ({len(df)} samples):\n")
        correct = 0

        for idx, sample in tqdm(df.iterrows(), total=len(df), desc=f"{task}-en"):
            is_correct = False
            sample_log = {'sample_id': idx, 'ground_truth': {}, 'prediction_log': {}}

            if task == "EA":
                scenario = sample["scenario"]
                subject = sample["subject"]
                choices = sample["choices"]
                prompt = f"Scenario: {scenario}\nAs {subject}, how would you respond?\nChoices:\n"
                for i, choice in enumerate(choices):
                    prompt += f"{chr(97+i)}) {choice}\n"
                prompt += "Answer:"
                
                prediction_log = model.predict_choice(prompt)
                raw_pred = prediction_log['cleaned_response']
                gt = sample['label']
                sample_log['ground_truth'] = {'label': gt}
                sample_log['prediction_log'] = prediction_log

                choices_list = sample['choices']
                letter_to_choice_map = {chr(97 + i): choice.strip() for i, choice in enumerate(choices_list)}

                model_selected_letter = re.search(r"([a-d])", raw_pred, re.IGNORECASE)
                pred = raw_pred
                if model_selected_letter and model_selected_letter.group(1).lower() in letter_to_choice_map:
                    pred = letter_to_choice_map[model_selected_letter.group(1).lower()]
                else:
                    for choice_text in letter_to_choice_map.values():
                        if normalize_text(raw_pred).startswith(normalize_text(choice_text)):
                            pred = choice_text
                            break
                
                is_correct = normalize_text(pred) == normalize_text(gt)

            elif task == "EU":
                situation = sample['scenario']
                gt_emo_label = sample['emotion_label']
                sample_log['ground_truth'] = {'emotion': gt_emo_label}

                prediction_log = model.predict_proba(situation)
                sample_log['prediction_log'] = prediction_log

                model_emo_label = ""
                if prediction_log['probabilities']:
                    model_emo_label = max(prediction_log['probabilities'], key=prediction_log['probabilities'].get)

                is_correct = normalize_text(model_emo_label) == normalize_text(gt_emo_label)

            sample_log['is_correct'] = is_correct
            if is_correct:
                correct += 1
            
            run_logs.append(sample_log)

        # --- Save Logs for the current task ---
        log_filename = LOG_DIR / f"{task}_results_lora_{timestamp}.json"
        with open(log_filename, 'w') as f:
            json.dump(run_logs, f, indent=2)
        print(f"Saved detailed logs to {log_filename}")

        accuracy = correct / len(df) if len(df) > 0 else 0
        task_accuracies[(task, "en")] = accuracy
        print(f"\n{task}-en Accuracy: {correct}/{len(df)} = {accuracy:.4f}")

    print("\n--- Overall Model Scores (with LoRA CPTs) ---")
    if task_accuracies:
        for (task_name, lang_name), acc in task_accuracies.items():
            print(f"  BirdModel - {task_name}-{lang_name} Accuracy: {acc:.4f}")
    else:
        print("  No tasks were evaluated to show a summary.")
    print("\nDone.")
