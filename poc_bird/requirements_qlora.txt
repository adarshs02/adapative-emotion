# QLoRA Fine-tuning Requirements for Llama 3.1 Embedding Training
# Optimized for memory-efficient training with quantization

# Core ML libraries
torch>=2.0.0
transformers>=4.35.0
tokenizers>=0.14.0
accelerate>=0.24.0

# QLoRA and PEFT
peft>=0.6.0
bitsandbytes>=0.41.0

# Dataset and evaluation
datasets>=2.14.0
scikit-learn>=1.3.0
numpy>=1.24.0

# Training utilities
wandb>=0.15.0
tqdm>=4.65.0

# CUDA support (if available)
xformers>=0.0.22  # For memory efficiency
flash-attn>=2.3.0  # Optional: for faster attention

# Utilities
packaging>=21.0
psutil>=5.9.0
